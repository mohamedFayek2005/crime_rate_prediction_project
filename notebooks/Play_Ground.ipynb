{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: scikit-learn-extra in f:\\anaconda\\anaconda after setup\\lib\\site-packages (0.3.0)\n",
      "Requirement already satisfied: numpy>=1.13.3 in f:\\anaconda\\anaconda after setup\\lib\\site-packages (from scikit-learn-extra) (1.26.4)\n",
      "Requirement already satisfied: scipy>=0.19.1 in f:\\anaconda\\anaconda after setup\\lib\\site-packages (from scikit-learn-extra) (1.13.1)\n",
      "Requirement already satisfied: scikit-learn>=0.23.0 in f:\\anaconda\\anaconda after setup\\lib\\site-packages (from scikit-learn-extra) (1.4.2)\n",
      "Requirement already satisfied: joblib>=1.2.0 in f:\\anaconda\\anaconda after setup\\lib\\site-packages (from scikit-learn>=0.23.0->scikit-learn-extra) (1.4.2)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in f:\\anaconda\\anaconda after setup\\lib\\site-packages (from scikit-learn>=0.23.0->scikit-learn-extra) (2.2.0)\n",
      "Requirement already satisfied: imbalanced-learn in f:\\anaconda\\anaconda after setup\\lib\\site-packages (0.12.3)\n",
      "Collecting imbalanced-learn\n",
      "  Downloading imbalanced_learn-0.13.0-py3-none-any.whl.metadata (8.8 kB)\n",
      "Requirement already satisfied: numpy<3,>=1.24.3 in f:\\anaconda\\anaconda after setup\\lib\\site-packages (from imbalanced-learn) (1.26.4)\n",
      "Requirement already satisfied: scipy<2,>=1.10.1 in f:\\anaconda\\anaconda after setup\\lib\\site-packages (from imbalanced-learn) (1.13.1)\n",
      "Requirement already satisfied: scikit-learn<2,>=1.3.2 in f:\\anaconda\\anaconda after setup\\lib\\site-packages (from imbalanced-learn) (1.4.2)\n",
      "Collecting sklearn-compat<1,>=0.1 (from imbalanced-learn)\n",
      "  Downloading sklearn_compat-0.1.3-py3-none-any.whl.metadata (18 kB)\n",
      "Requirement already satisfied: joblib<2,>=1.1.1 in f:\\anaconda\\anaconda after setup\\lib\\site-packages (from imbalanced-learn) (1.4.2)\n",
      "Requirement already satisfied: threadpoolctl<4,>=2.0.0 in f:\\anaconda\\anaconda after setup\\lib\\site-packages (from imbalanced-learn) (2.2.0)\n",
      "Downloading imbalanced_learn-0.13.0-py3-none-any.whl (238 kB)\n",
      "   ---------------------------------------- 0.0/238.4 kB ? eta -:--:--\n",
      "   ---------- ----------------------------- 61.4/238.4 kB 1.7 MB/s eta 0:00:01\n",
      "   ---------------------------------- ----- 204.8/238.4 kB 2.5 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 238.4/238.4 kB 2.1 MB/s eta 0:00:00\n",
      "Downloading sklearn_compat-0.1.3-py3-none-any.whl (18 kB)\n",
      "Installing collected packages: sklearn-compat, imbalanced-learn\n",
      "  Attempting uninstall: imbalanced-learn\n",
      "    Found existing installation: imbalanced-learn 0.12.3\n",
      "    Uninstalling imbalanced-learn-0.12.3:\n",
      "      Successfully uninstalled imbalanced-learn-0.12.3\n",
      "Successfully installed imbalanced-learn-0.13.0 sklearn-compat-0.1.3\n"
     ]
    }
   ],
   "source": [
    "!pip install scikit-learn-extra # For K_medoids\n",
    "!pip install -U imbalanced-learn # For Oversampling and Undersampling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import pandas as pd\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn_extra.cluster import KMedoids\n",
    "from sklearn.metrics import silhouette_score\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import classification_report, accuracy_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv(\"../data/preprocessed_data.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<bound method DataFrame.info of         Category  DayOfWeek  PdDistrict         X         Y       Day  \\\n",
       "0             37  -0.503387           4 -0.107902  0.007832 -0.292682   \n",
       "1             21  -0.503387           4 -0.107902  0.007832 -0.292682   \n",
       "2             21  -0.503387           4 -0.057541  0.064335 -0.292682   \n",
       "3             16  -0.503387           4 -0.144262  0.065338 -0.292682   \n",
       "4             16  -0.503387           5 -0.531112  0.001140 -0.292682   \n",
       "...          ...        ...         ...       ...       ...       ...   \n",
       "878044        25  -1.517575           8 -1.199747 -0.124677 -1.089676   \n",
       "878045        16  -1.517575           2 -0.815292 -0.085518 -1.089676   \n",
       "878046        16  -1.517575           7  0.633404  0.020235 -1.089676   \n",
       "878047        35  -1.517575           7  1.057042  0.020983 -1.089676   \n",
       "878048        12  -1.517575           0  0.912272 -0.071808 -1.089676   \n",
       "\n",
       "           Month     Year      Hour    Minute     Block  \n",
       "0      -0.418933  1.73165  1.463813  1.766343 -1.538350  \n",
       "1      -0.418933  1.73165  1.463813  1.766343 -1.538350  \n",
       "2      -0.418933  1.73165  1.463813  0.690779 -1.538350  \n",
       "3      -0.418933  1.73165  1.463813  0.529445  0.650047  \n",
       "4      -0.418933  1.73165  1.463813  0.529445  0.650047  \n",
       "...          ...      ...       ...       ...       ...  \n",
       "878044 -1.585464 -1.57305 -2.047868 -0.277228 -1.538350  \n",
       "878045 -1.585464 -1.57305 -2.047868 -1.030122  0.650047  \n",
       "878046 -1.585464 -1.57305 -2.047868 -1.030122 -1.538350  \n",
       "878047 -1.585464 -1.57305 -2.047868 -1.030122 -1.538350  \n",
       "878048 -1.585464 -1.57305 -2.047868 -1.030122  0.650047  \n",
       "\n",
       "[878049 rows x 11 columns]>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.info"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# KMeans\n",
    "#### KMeans is an unsupervised algorithm, so we cannot apply GridSearchCV directly \n",
    "#### since it requires labeled data (supervised evaluation)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cluster_data(df, features=None, k_range=range(2, 11), max_samples=10000, random_state=42, verbose=True):\n",
    "    \n",
    "    \n",
    "    #data = df[features].copy()\n",
    "    \n",
    "    # Sample Of Data\n",
    "    sample_size = min(len(df), max_samples)\n",
    "    samples = df.sample(n=sample_size, random_state=random_state)  \n",
    "    \n",
    "    # Search for best K\n",
    "    scores = []\n",
    "    labels_dict = {}\n",
    "    \n",
    "    # LOOP\n",
    "    for k in k_range:\n",
    "    \n",
    "        kmedoids = KMedoids(n_clusters=k, random_state=random_state, metric='euclidean')\n",
    "        labels = kmedoids.fit_predict(samples)\n",
    "        labels_dict[k] = labels\n",
    "        medoids_dict = {k: kmedoids}\n",
    "        \n",
    "        #silhouette score\n",
    "        score = silhouette_score(samples, labels)\n",
    "        scores.append(score)\n",
    "    \n",
    "    # Find optimal k (skip k=1 if it's in the range)\n",
    "    best_k = k_range[scores.index(max(scores))]\n",
    "\n",
    "    df_clustered = df.copy()\n",
    "\n",
    "    # Add Cluster Column to Data\n",
    "    df_clustered.loc[samples.index, 'Cluster'] = labels_dict[best_k] \n",
    "    \n",
    "    # Get the best model\n",
    "    best_kmedoids = KMedoids(n_clusters=best_k, random_state=random_state).fit(samples)\n",
    "    \n",
    "    if len(df_clustered) > sample_size:\n",
    "\n",
    "        # Assign remaining points to nearest medoid center\n",
    "        remaining_indices = df_clustered.index.difference(samples.index)\n",
    "        remaining_data = df.loc[remaining_indices]\n",
    "\n",
    "        # Predict clusters for remaining points\n",
    "        remaining_labels = best_kmedoids.predict(remaining_data)\n",
    "        df_clustered.loc[remaining_indices, 'Cluster'] = remaining_labels\n",
    "    \n",
    "    return df_clustered, best_k, scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyze_clusters(df, cluster_col='Cluster'):\n",
    "\n",
    "    features = df.select_dtypes(include='number').columns.drop(cluster_col)\n",
    "\n",
    "    print(df[cluster_col].value_counts().sort_index())\n",
    "    return df.groupby(cluster_col)[features].mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cluster\n",
      "0.0    185812\n",
      "1.0    188943\n",
      "2.0    143248\n",
      "3.0    360046\n",
      "Name: count, dtype: int64\n",
      "         DayOfWeek  PdDistrict         X         Y       Day     Month  \\\n",
      "Cluster                                                                  \n",
      "0.0      -0.068696    3.318650 -0.118703 -0.012461  0.106721 -0.456932   \n",
      "1.0       0.123613    0.677501  0.611560 -0.014515  0.011297 -0.123174   \n",
      "2.0      -0.041644    3.317819 -0.127199 -0.013012 -0.141354  0.805412   \n",
      "3.0      -0.012848    7.452589 -0.209064  0.019225 -0.004766 -0.019990   \n",
      "\n",
      "             Year      Hour    Minute     Block  \n",
      "Cluster                                          \n",
      "0.0      0.419003  0.184235  0.432122 -0.083949  \n",
      "1.0     -0.155452  0.022396 -0.106612  0.021070  \n",
      "2.0     -0.299146 -0.310279 -0.449927 -0.048416  \n",
      "3.0     -0.015643  0.016615  0.011947  0.051530  \n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 878049 entries, 0 to 878048\n",
      "Data columns (total 11 columns):\n",
      " #   Column      Non-Null Count   Dtype  \n",
      "---  ------      --------------   -----  \n",
      " 0   DayOfWeek   878049 non-null  float64\n",
      " 1   PdDistrict  878049 non-null  int64  \n",
      " 2   X           878049 non-null  float64\n",
      " 3   Y           878049 non-null  float64\n",
      " 4   Day         878049 non-null  float64\n",
      " 5   Month       878049 non-null  float64\n",
      " 6   Year        878049 non-null  float64\n",
      " 7   Hour        878049 non-null  float64\n",
      " 8   Minute      878049 non-null  float64\n",
      " 9   Block       878049 non-null  float64\n",
      " 10  Cluster     878049 non-null  float64\n",
      "dtypes: float64(10), int64(1)\n",
      "memory usage: 73.7 MB\n"
     ]
    }
   ],
   "source": [
    "X = data.drop(columns=['Category'])  # Numerical Data\n",
    "Y = data['Category']\n",
    "\n",
    "df_clustered, best_k, scores = cluster_data(X)  \n",
    "\n",
    "\n",
    "# Analyze cluster characteristics\n",
    "cluster_means = analyze_clusters(df_clustered)\n",
    "print(cluster_means)\n",
    "\n",
    "df_clustered.info()\n",
    "\n",
    "#0.0    185812\n",
    "#1.0    188943\n",
    "#2.0    143248\n",
    "#3.0    360046\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### RandomForestClassifier "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_clustered[\"Category\"]=data[\"Category\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_sampled = df_clustered.sample(50000, random_state=42)\n",
    "\n",
    "# Separate features and target\n",
    "X = df_sampled.drop(columns=['Category'])\n",
    "y = df_sampled['Category']\n",
    "# Train/test split\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.05, random_state=42, stratify=y)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.2688\n",
      "\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.00      0.00      0.00         5\n",
      "           1       0.16      0.16      0.16       219\n",
      "           2       0.00      0.00      0.00         1\n",
      "           3       0.00      0.00      0.00         1\n",
      "           4       0.17      0.10      0.13       106\n",
      "           5       0.50      0.09      0.15        11\n",
      "           6       0.00      0.00      0.00         6\n",
      "           7       0.35      0.41      0.37       155\n",
      "           8       0.00      0.00      0.00        12\n",
      "           9       0.00      0.00      0.00         2\n",
      "          10       0.00      0.00      0.00         1\n",
      "          11       0.00      0.00      0.00         1\n",
      "          12       0.00      0.00      0.00        30\n",
      "          13       0.06      0.02      0.03        47\n",
      "          14       0.00      0.00      0.00         1\n",
      "          15       0.00      0.00      0.00         7\n",
      "          16       0.32      0.64      0.43       496\n",
      "          17       0.00      0.00      0.00         6\n",
      "          18       0.00      0.00      0.00         4\n",
      "          19       0.29      0.16      0.21        73\n",
      "          20       0.20      0.17      0.19       264\n",
      "          21       0.28      0.37      0.32       355\n",
      "          23       0.69      0.50      0.58        22\n",
      "          24       0.00      0.00      0.00         9\n",
      "          25       0.05      0.02      0.02        65\n",
      "          26       0.00      0.00      0.00         6\n",
      "          27       0.00      0.00      0.00        30\n",
      "          28       0.00      0.00      0.00        13\n",
      "          30       0.00      0.00      0.00        14\n",
      "          31       0.00      0.00      0.00         1\n",
      "          32       0.12      0.03      0.05        90\n",
      "          34       0.50      0.05      0.08        22\n",
      "          35       0.07      0.03      0.04       129\n",
      "          36       0.28      0.21      0.24       151\n",
      "          37       0.11      0.04      0.06       120\n",
      "          38       0.00      0.00      0.00        25\n",
      "\n",
      "    accuracy                           0.27      2500\n",
      "   macro avg       0.12      0.08      0.09      2500\n",
      "weighted avg       0.22      0.27      0.23      2500\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "f:\\Anaconda\\Anaconda After SetUp\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "f:\\Anaconda\\Anaconda After SetUp\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "f:\\Anaconda\\Anaconda After SetUp\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
     ]
    }
   ],
   "source": [
    "model = RandomForestClassifier(random_state=42)\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# Predict\n",
    "y_pred = model.predict(X_test)\n",
    "\n",
    "# Evaluate\n",
    "print(\"Accuracy:\", accuracy_score(y_test, y_pred))\n",
    "print(\"\\nClassification Report:\\n\", classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SMOTE Techniuqe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from imblearn.over_sampling import SMOTE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'df_clustered' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[2], line 26\u001b[0m\n\u001b[0;32m     23\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNew shape: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mX_resampled\u001b[38;5;241m.\u001b[39mshape\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     25\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m X_resampled, y_resampled\n\u001b[1;32m---> 26\u001b[0m X_balanced, y_balanced \u001b[38;5;241m=\u001b[39m apply_smote(\u001b[43mdf_clustered\u001b[49m, target_column\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mCategory\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m     28\u001b[0m \u001b[38;5;66;03m# If you want to get a complete balanced DataFrame\u001b[39;00m\n\u001b[0;32m     29\u001b[0m df \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mconcat([X_balanced, y_balanced], axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'df_clustered' is not defined"
     ]
    }
   ],
   "source": [
    "def apply_smote(df, target_column='Category', random_state=42):\n",
    "\n",
    "    # Separate features and target\n",
    "    X = df.drop(columns=[target_column])\n",
    "    y = df[target_column]\n",
    "    \n",
    "    # Display original class distribution\n",
    "    print(\"Original class distribution:\")\n",
    "    print(y.value_counts())\n",
    "    print(f\"Original shape: {X.shape}\")\n",
    "    \n",
    "    # Apply SMOTE\n",
    "    smote = SMOTE(random_state=random_state)\n",
    "    X_resampled, y_resampled = smote.fit_resample(X, y)\n",
    "    \n",
    "    # Convert back to DataFrame/Series to maintain column names\n",
    "    X_resampled = pd.DataFrame(X_resampled, columns=X.columns)\n",
    "    y_resampled = pd.Series(y_resampled, name=target_column)\n",
    "    \n",
    "    # Display new class distribution\n",
    "    print(\"\\nBalanced class distribution after SMOTE:\")\n",
    "    print(y_resampled.value_counts())\n",
    "    print(f\"New shape: {X_resampled.shape}\")\n",
    "    \n",
    "    return X_resampled, y_resampled\n",
    "X_balanced, y_balanced = apply_smote(df_clustered, target_column='Category')\n",
    "\n",
    "# If you want to get a complete balanced DataFrame\n",
    "df = pd.concat([X_balanced, y_balanced], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'df' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m df_sampled \u001b[38;5;241m=\u001b[39m \u001b[43mdf\u001b[49m\u001b[38;5;241m.\u001b[39msample(\u001b[38;5;241m50000\u001b[39m, random_state\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m42\u001b[39m)\n\u001b[0;32m      3\u001b[0m \u001b[38;5;66;03m# Separate features and target\u001b[39;00m\n\u001b[0;32m      4\u001b[0m X \u001b[38;5;241m=\u001b[39m df_sampled\u001b[38;5;241m.\u001b[39mdrop(columns\u001b[38;5;241m=\u001b[39m[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mCategory\u001b[39m\u001b[38;5;124m'\u001b[39m])\n",
      "\u001b[1;31mNameError\u001b[0m: name 'df' is not defined"
     ]
    }
   ],
   "source": [
    "df_sampled = df.sample(50000, random_state=42)\n",
    "\n",
    "# Separate features and target\n",
    "X = df_sampled.drop(columns=['Category'])\n",
    "y = df_sampled['Category']\n",
    "# Train/test split\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.05, random_state=42, stratify=y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['DayOfWeek', 'PdDistrict', 'X', 'Y', 'Day', 'Month', 'Year', 'Hour',\n",
       "       'Minute', 'Block', 'Cluster'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<bound method NDFrame.head of 1893628     6\n",
       "2270057     8\n",
       "3815608    19\n",
       "4667528    25\n",
       "2755976    11\n",
       "           ..\n",
       "6671538    38\n",
       "5893134    32\n",
       "866041     35\n",
       "3245048    14\n",
       "5747569    31\n",
       "Name: Category, Length: 760000, dtype: int64>"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train.head"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "39"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(y_train.value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.707025\n",
      "\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.80      0.94      0.87      1024\n",
      "           1       0.35      0.20      0.26      1037\n",
      "           2       0.90      1.00      0.94      1029\n",
      "           3       0.91      1.00      0.95      1027\n",
      "           4       0.42      0.42      0.42      1025\n",
      "           5       0.68      0.74      0.71      1029\n",
      "           6       0.73      0.94      0.82      1023\n",
      "           7       0.44      0.55      0.49      1025\n",
      "           8       0.66      0.79      0.72      1028\n",
      "           9       0.83      0.97      0.89      1032\n",
      "          10       0.92      1.00      0.96      1024\n",
      "          11       0.90      0.99      0.94      1024\n",
      "          12       0.62      0.68      0.65      1016\n",
      "          13       0.57      0.50      0.54      1023\n",
      "          14       0.95      1.00      0.97      1034\n",
      "          15       0.77      0.84      0.80      1025\n",
      "          16       0.35      0.30      0.32      1023\n",
      "          17       0.78      0.91      0.84      1018\n",
      "          18       0.84      0.94      0.89      1020\n",
      "          19       0.62      0.59      0.60      1025\n",
      "          20       0.34      0.20      0.25      1030\n",
      "          21       0.30      0.20      0.24      1031\n",
      "          22       1.00      1.00      1.00      1026\n",
      "          23       0.82      0.91      0.86      1029\n",
      "          24       0.69      0.91      0.78      1021\n",
      "          25       0.54      0.43      0.48      1029\n",
      "          26       0.83      0.93      0.88      1023\n",
      "          27       0.61      0.53      0.57      1018\n",
      "          28       0.75      0.78      0.76      1017\n",
      "          29       0.95      1.00      0.98      1027\n",
      "          30       0.68      0.77      0.72      1028\n",
      "          31       0.90      0.99      0.94      1029\n",
      "          32       0.51      0.29      0.37      1038\n",
      "          33       1.00      1.00      1.00      1015\n",
      "          34       0.63      0.64      0.64      1023\n",
      "          35       0.46      0.27      0.34      1024\n",
      "          36       0.49      0.51      0.50      1032\n",
      "          37       0.41      0.29      0.34      1022\n",
      "          38       0.72      0.65      0.68      1027\n",
      "\n",
      "    accuracy                           0.71     40000\n",
      "   macro avg       0.68      0.71      0.69     40000\n",
      "weighted avg       0.68      0.71      0.69     40000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "model = RandomForestClassifier(random_state=42)\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# Predict\n",
    "y_pred = model.predict(X_test)\n",
    "\n",
    "# Evaluate\n",
    "print(\"Accuracy:\", accuracy_score(y_test, y_pred))\n",
    "print(\"\\nClassification Report:\\n\", classification_report(y_test, y_pred))\n",
    "\n",
    "\n",
    "# Sampel Size -> Accuracy\n",
    "#   800,000   -> Accuracy: 0.707025"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Great , Accuracy jumped from 25% to 70% after applying SMOTE! "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting fuzzy-c-means\n",
      "  Downloading fuzzy_c_means-1.7.2-py3-none-any.whl.metadata (4.7 kB)\n",
      "Requirement already satisfied: joblib<2.0.0,>=1.2.0 in f:\\anaconda\\anaconda after setup\\lib\\site-packages (from fuzzy-c-means) (1.4.2)\n",
      "Requirement already satisfied: numpy<2.0.0,>=1.21.1 in f:\\anaconda\\anaconda after setup\\lib\\site-packages (from fuzzy-c-means) (1.26.4)\n",
      "Collecting pydantic<3.0.0,>=2.6.4 (from fuzzy-c-means)\n",
      "  Downloading pydantic-2.11.4-py3-none-any.whl.metadata (66 kB)\n",
      "     ---------------------------------------- 0.0/66.6 kB ? eta -:--:--\n",
      "     ------------------------------------ --- 61.4/66.6 kB 1.1 MB/s eta 0:00:01\n",
      "     -------------------------------------- 66.6/66.6 kB 894.7 kB/s eta 0:00:00\n",
      "Collecting tabulate<0.9.0,>=0.8.9 (from fuzzy-c-means)\n",
      "  Downloading tabulate-0.8.10-py3-none-any.whl.metadata (25 kB)\n",
      "Requirement already satisfied: tqdm<5.0.0,>=4.64.1 in f:\\anaconda\\anaconda after setup\\lib\\site-packages (from fuzzy-c-means) (4.66.4)\n",
      "Collecting typer<0.10.0,>=0.9.0 (from fuzzy-c-means)\n",
      "  Downloading typer-0.9.4-py3-none-any.whl.metadata (14 kB)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in f:\\anaconda\\anaconda after setup\\lib\\site-packages (from pydantic<3.0.0,>=2.6.4->fuzzy-c-means) (0.6.0)\n",
      "Collecting pydantic-core==2.33.2 (from pydantic<3.0.0,>=2.6.4->fuzzy-c-means)\n",
      "  Downloading pydantic_core-2.33.2-cp312-cp312-win_amd64.whl.metadata (6.9 kB)\n",
      "Collecting typing-extensions>=4.12.2 (from pydantic<3.0.0,>=2.6.4->fuzzy-c-means)\n",
      "  Using cached typing_extensions-4.13.2-py3-none-any.whl.metadata (3.0 kB)\n",
      "Collecting typing-inspection>=0.4.0 (from pydantic<3.0.0,>=2.6.4->fuzzy-c-means)\n",
      "  Downloading typing_inspection-0.4.0-py3-none-any.whl.metadata (2.6 kB)\n",
      "Requirement already satisfied: colorama in c:\\users\\kimostore\\appdata\\roaming\\python\\python312\\site-packages (from tqdm<5.0.0,>=4.64.1->fuzzy-c-means) (0.4.6)\n",
      "Requirement already satisfied: click<9.0.0,>=7.1.1 in f:\\anaconda\\anaconda after setup\\lib\\site-packages (from typer<0.10.0,>=0.9.0->fuzzy-c-means) (8.1.7)\n",
      "Downloading fuzzy_c_means-1.7.2-py3-none-any.whl (9.1 kB)\n",
      "Downloading pydantic-2.11.4-py3-none-any.whl (443 kB)\n",
      "   ---------------------------------------- 0.0/443.9 kB ? eta -:--:--\n",
      "   --------- ------------------------------ 102.4/443.9 kB 2.0 MB/s eta 0:00:01\n",
      "   ----------------------- ---------------- 256.0/443.9 kB 2.6 MB/s eta 0:00:01\n",
      "   -------------------------------------- - 430.1/443.9 kB 3.4 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 443.9/443.9 kB 3.1 MB/s eta 0:00:00\n",
      "Downloading pydantic_core-2.33.2-cp312-cp312-win_amd64.whl (2.0 MB)\n",
      "   ---------------------------------------- 0.0/2.0 MB ? eta -:--:--\n",
      "   --- ------------------------------------ 0.2/2.0 MB 3.3 MB/s eta 0:00:01\n",
      "   ------- -------------------------------- 0.4/2.0 MB 3.7 MB/s eta 0:00:01\n",
      "   ---------- ----------------------------- 0.5/2.0 MB 3.9 MB/s eta 0:00:01\n",
      "   -------------- ------------------------- 0.7/2.0 MB 3.9 MB/s eta 0:00:01\n",
      "   ---------------- ----------------------- 0.8/2.0 MB 3.7 MB/s eta 0:00:01\n",
      "   -------------------- ------------------- 1.0/2.0 MB 3.8 MB/s eta 0:00:01\n",
      "   ------------------------ --------------- 1.2/2.0 MB 3.8 MB/s eta 0:00:01\n",
      "   ---------------------------- ----------- 1.4/2.0 MB 3.8 MB/s eta 0:00:01\n",
      "   ------------------------------ --------- 1.5/2.0 MB 3.9 MB/s eta 0:00:01\n",
      "   ---------------------------------- ----- 1.7/2.0 MB 3.8 MB/s eta 0:00:01\n",
      "   -------------------------------------- - 1.9/2.0 MB 3.8 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 2.0/2.0 MB 3.8 MB/s eta 0:00:00\n",
      "Downloading tabulate-0.8.10-py3-none-any.whl (29 kB)\n",
      "Downloading typer-0.9.4-py3-none-any.whl (45 kB)\n",
      "   ---------------------------------------- 0.0/46.0 kB ? eta -:--:--\n",
      "   ---------------------------------------- 46.0/46.0 kB 2.4 MB/s eta 0:00:00\n",
      "Using cached typing_extensions-4.13.2-py3-none-any.whl (45 kB)\n",
      "Downloading typing_inspection-0.4.0-py3-none-any.whl (14 kB)\n",
      "Installing collected packages: typing-extensions, tabulate, typing-inspection, typer, pydantic-core, pydantic, fuzzy-c-means\n",
      "  Attempting uninstall: typing-extensions\n",
      "    Found existing installation: typing_extensions 4.11.0\n",
      "    Uninstalling typing_extensions-4.11.0:\n",
      "      Successfully uninstalled typing_extensions-4.11.0\n",
      "  Attempting uninstall: tabulate\n",
      "    Found existing installation: tabulate 0.9.0\n",
      "    Uninstalling tabulate-0.9.0:\n",
      "      Successfully uninstalled tabulate-0.9.0\n",
      "  Attempting uninstall: pydantic-core\n",
      "    Found existing installation: pydantic_core 2.14.6\n",
      "    Uninstalling pydantic_core-2.14.6:\n",
      "      Successfully uninstalled pydantic_core-2.14.6\n",
      "  Attempting uninstall: pydantic\n",
      "    Found existing installation: pydantic 2.5.3\n",
      "    Uninstalling pydantic-2.5.3:\n",
      "      Successfully uninstalled pydantic-2.5.3\n",
      "Successfully installed fuzzy-c-means-1.7.2 pydantic-2.11.4 pydantic-core-2.33.2 tabulate-0.8.10 typer-0.9.4 typing-extensions-4.13.2 typing-inspection-0.4.0\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "streamlit 1.32.0 requires packaging<24,>=16.8, but you have packaging 24.1 which is incompatible.\n"
     ]
    }
   ],
   "source": [
    "pip install fuzzy-c-means"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "from fcmeans import FCM\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "from skfuzzy import cmeans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cluster_data(df, features=None, k_range=range(2, 11), max_samples=10000, verbose=True):\n",
    "    \n",
    "    # Sample of Data\n",
    "    sample_size = min(len(df), max_samples)\n",
    "    samples = df.sample(n=sample_size)  \n",
    "    \n",
    "    # Search for best K\n",
    "    scores = []\n",
    "    labels_dict = {}\n",
    "    \n",
    "    # Loop through k_range\n",
    "    for k in k_range:\n",
    "        \n",
    "        # Fuzzy C-Means clustering\n",
    "        cntr, u, u0, d, jm, p, fpc = cmeans(samples.values.T, k, 2, error=0.005, maxiter=1000, init=None)\n",
    "        \n",
    "        # Fuzzy labels (taking the highest membership value for each point)\n",
    "        labels = np.argmax(u, axis=0)\n",
    "        labels_dict[k] = labels\n",
    "        \n",
    "        # Silhouette score\n",
    "        score = silhouette_score(samples, labels)\n",
    "        scores.append(score)\n",
    "    \n",
    "    # Find optimal k\n",
    "    best_k = k_range[scores.index(max(scores))]\n",
    "\n",
    "    df_clustered = df.copy()\n",
    "\n",
    "    # Add fuzzy cluster column to data\n",
    "    df_clustered.loc[samples.index, 'fuzzy_cluster'] = labels_dict[best_k]\n",
    "    \n",
    "    # Get the best fuzzy model (for assigning the rest of the data)\n",
    "    cntr_best, u_best, u0_best, d_best, jm_best, p_best, fpc_best = cmeans(samples.values.T, best_k, 2, error=0.005, maxiter=1000, init=None)\n",
    "    \n",
    "    if len(df_clustered) > sample_size:\n",
    "        # Assign remaining points to nearest fuzzy cluster\n",
    "        remaining_indices = df_clustered.index.difference(samples.index)\n",
    "        remaining_data = df.loc[remaining_indices]\n",
    "        \n",
    "        # Predict fuzzy clusters for remaining points (based on the highest membership)\n",
    "        remaining_u = np.argmax(cmeans(remaining_data.values.T, best_k, 2, error=0.005, maxiter=1000, init=None)[1], axis=0)\n",
    "        df_clustered.loc[remaining_indices, 'fuzzy_cluster'] = remaining_u\n",
    "    \n",
    "    return df_clustered, best_k, scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df.drop(columns=['Category'])  # Numerical Data\n",
    "Y = df['Category']\n",
    "\n",
    "df_clustered, best_k, scores = cluster_data(X)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['DayOfWeek', 'PdDistrict', 'X', 'Y', 'Day', 'Month', 'Year', 'Hour',\n",
       "       'Minute', 'Block', 'Cluster', 'fuzzy_cluster'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_clustered.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "Data=df_clustered\n",
    "Data[\"Category\"]=Y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['DayOfWeek', 'PdDistrict', 'X', 'Y', 'Day', 'Month', 'Year', 'Hour',\n",
       "       'Minute', 'Block', 'Cluster', 'fuzzy_cluster', 'Category'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Data.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_sampled_2 = Data.sample(500000, random_state=42)\n",
    "y = df_sampled_2['Category']\n",
    "X = df_sampled_2.drop(columns=['Category', 'Cluster'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.05, random_state=42, stratify=y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['DayOfWeek', 'PdDistrict', 'X', 'Y', 'Day', 'Month', 'Year', 'Hour',\n",
       "       'Minute', 'Block', 'fuzzy_cluster'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<bound method NDFrame.head of 2996139    13\n",
       "2063646     7\n",
       "5832339    32\n",
       "3816104    19\n",
       "6763797    38\n",
       "           ..\n",
       "3424363    15\n",
       "2675466    11\n",
       "6067171    33\n",
       "4088968    21\n",
       "2781776    11\n",
       "Name: Category, Length: 475000, dtype: int64>"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train.head"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.6552\n",
      "\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.75      0.92      0.83       640\n",
      "           1       0.27      0.16      0.20       644\n",
      "           2       0.84      0.99      0.91       639\n",
      "           3       0.85      0.99      0.92       639\n",
      "           4       0.37      0.36      0.36       638\n",
      "           5       0.64      0.64      0.64       643\n",
      "           6       0.65      0.90      0.75       644\n",
      "           7       0.38      0.48      0.43       640\n",
      "           8       0.60      0.70      0.64       646\n",
      "           9       0.80      0.93      0.86       644\n",
      "          10       0.87      1.00      0.93       638\n",
      "          11       0.85      0.98      0.91       635\n",
      "          12       0.56      0.55      0.55       638\n",
      "          13       0.48      0.41      0.44       645\n",
      "          14       0.93      1.00      0.96       646\n",
      "          15       0.69      0.77      0.73       638\n",
      "          16       0.30      0.26      0.28       645\n",
      "          17       0.70      0.88      0.78       642\n",
      "          18       0.78      0.91      0.84       640\n",
      "          19       0.54      0.55      0.54       635\n",
      "          20       0.28      0.16      0.20       646\n",
      "          21       0.22      0.14      0.17       641\n",
      "          22       0.99      1.00      0.99       636\n",
      "          23       0.75      0.89      0.82       646\n",
      "          24       0.61      0.85      0.71       638\n",
      "          25       0.46      0.33      0.38       637\n",
      "          26       0.80      0.90      0.85       643\n",
      "          27       0.54      0.43      0.48       645\n",
      "          28       0.67      0.68      0.67       640\n",
      "          29       0.92      1.00      0.96       644\n",
      "          30       0.59      0.63      0.61       639\n",
      "          31       0.84      0.99      0.91       645\n",
      "          32       0.44      0.24      0.31       642\n",
      "          33       1.00      1.00      1.00       636\n",
      "          34       0.56      0.54      0.55       637\n",
      "          35       0.35      0.19      0.25       637\n",
      "          36       0.47      0.46      0.46       640\n",
      "          37       0.36      0.24      0.29       646\n",
      "          38       0.62      0.54      0.58       643\n",
      "\n",
      "    accuracy                           0.66     25000\n",
      "   macro avg       0.62      0.66      0.63     25000\n",
      "weighted avg       0.62      0.66      0.63     25000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "model = RandomForestClassifier(random_state=42)\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# Predict\n",
    "y_pred = model.predict(X_test)\n",
    "\n",
    "# Evaluate\n",
    "print(\"Accuracy:\", accuracy_score(y_test, y_pred))\n",
    "print(\"\\nClassification Report:\\n\", classification_report(y_test, y_pred))\n",
    "\n",
    "# Sampel Size -> Accuracy\n",
    "#   10,000    -> Accuracy: 0.30\n",
    "#   40,000    -> Accuracy: 0.39\n",
    "#   80,000    -> Accuracy: 0.47\n",
    "#   160,000   -> Accuracy: 0.53\n",
    "#   300,000   -> Accuracy: 0.60\n",
    "#   500,000   -> Accuracy: 0.65"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
